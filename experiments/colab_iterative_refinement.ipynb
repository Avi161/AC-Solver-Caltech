{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AC-Solver Iterative Refinement — Colab Runner\n",
    "\n",
    "**Runtime recommendation: High-RAM CPU**  \n",
    "The search (V-guided greedy, beam) is CPU-only. GPU only helps training, which takes ~10 min per iteration — not the bottleneck.\n",
    "\n",
    "**Normal workflow:**\n",
    "- **First time / clean restart:** Cells 1 → 2 → 3 → 4 → 5 → 6\n",
    "- **After Colab disconnect:** Cells 1 → 2 → 3 → 5 → 8 *(state persists on Drive)*\n",
    "- **If state files are missing but partial results exist:** Cells 1 → 2 → 3 → 5 → 7 → 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1 — Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Drive mounted.\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print('Drive mounted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2 — Clone or Update Repo on Drive\n",
    "\n",
    "**First time only:** enter your GitHub Personal Access Token when prompted.  \n",
    "Get one at: GitHub → Settings → Developer Settings → Personal access tokens → Fine-grained → repo read access.  \n",
    "After first clone, this cell just does `git pull`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating d7d1132..133042e\n",
      "Fast-forward\n",
      " experiments/colab_iterative_refinement.ipynb | 239 +++++++++++++++++++--------\n",
      " experiments/run_experiments.py               |  12 +-\n",
      " 2 files changed, 179 insertions(+), 72 deletions(-)\n",
      "\n",
      "Working directory: /content/drive/MyDrive/AC-Solver-Caltech\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "REPO_URL = 'https://github.com/Avi161/AC-Solver-Caltech.git'\n",
    "DRIVE_DIR = '/content/drive/MyDrive/AC-Solver-Caltech'\n",
    "BRANCH = 'feat/test-new-exp-claude'\n",
    "\n",
    "if not os.path.exists(DRIVE_DIR):\n",
    "    # First time: clone with token\n",
    "    from getpass import getpass\n",
    "    token = getpass('GitHub Personal Access Token: ')\n",
    "    auth_url = REPO_URL.replace('https://', f'https://{token}@')\n",
    "    subprocess.run(['git', 'clone', auth_url, DRIVE_DIR], check=True)\n",
    "    subprocess.run(['git', 'checkout', BRANCH], cwd=DRIVE_DIR, check=True)\n",
    "    print('Repo cloned to Drive.')\n",
    "else:\n",
    "    # Already cloned: checkout branch first, then pull\n",
    "    subprocess.run(['git', 'checkout', BRANCH], cwd=DRIVE_DIR, check=True)\n",
    "    result = subprocess.run(['git', 'pull'], cwd=DRIVE_DIR, capture_output=True, text=True)\n",
    "    print(result.stdout or 'Already up to date.')\n",
    "\n",
    "os.chdir(DRIVE_DIR)\n",
    "print(f'Working directory: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3 — Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "result = subprocess.run(\n",
    "    ['pip', 'install', '-q', '-r', 'requirements.txt'],\n",
    "    cwd=DRIVE_DIR, capture_output=True, text=True\n",
    ")\n",
    "print(result.stdout[-2000:] if result.stdout else 'Done.')\n",
    "if result.returncode != 0:\n",
    "    print('ERRORS:', result.stderr[-1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4 — Verify Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓  Original model checkpoint\n",
      "  ✓  Feature stats\n",
      "  ✓  Greedy solved presentations\n",
      "  ✓  Greedy search paths\n",
      "  ✓  Config\n",
      "\n",
      "  Saved state found: iteration=0, total_solved_history=[533]\n",
      "\n",
      "All checks passed. Ready to run.\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "\n",
    "checks = {\n",
    "    'Original model checkpoint':  'value_search/checkpoints/best_mlp.pt',\n",
    "    'Feature stats':              'value_search/checkpoints/feature_stats.json',\n",
    "    'Greedy solved presentations': 'ac_solver/search/miller_schupp/data/greedy_solved_presentations.txt',\n",
    "    'Greedy search paths':         'ac_solver/search/miller_schupp/data/greedy_search_paths.txt',\n",
    "    'Config':                      'experiments/config.yaml',\n",
    "}\n",
    "\n",
    "all_ok = True\n",
    "for label, path in checks.items():\n",
    "    full = os.path.join(DRIVE_DIR, path)\n",
    "    exists = os.path.exists(full)\n",
    "    status = '✓' if exists else '✗  MISSING'\n",
    "    print(f'  {status}  {label}')\n",
    "    if not exists:\n",
    "        all_ok = False\n",
    "\n",
    "# Check refinement state (if resuming)\n",
    "state_file = os.path.join(DRIVE_DIR, 'experiments/refinement/refinement_state.json')\n",
    "if os.path.exists(state_file):\n",
    "    with open(state_file) as f:\n",
    "        state = json.load(f)\n",
    "    print(f'\\n  Saved state found: iteration={state[\"iteration\"]}, '\n",
    "          f'total_solved_history={state[\"total_solved_per_iteration\"]}')\n",
    "else:\n",
    "    print('\\n  No saved state — will start fresh.')\n",
    "\n",
    "if all_ok:\n",
    "    print('\\nAll checks passed. Ready to run.')\n",
    "else:\n",
    "    print('\\nFix missing files before running.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5 — Configuration\n",
    "Edit these before running. Then run Cell 6 (fresh start) or Cell 8 (resume)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config set:\n",
      "  max_iterations  = 5\n",
      "  max_path_length = 800\n",
      "  enable_mcts     = False\n",
      "  max_nodes       = 100,000\n"
     ]
    }
   ],
   "source": [
    "# ── Tune these ────────────────────────────────────────────────────────────────\n",
    "MAX_ITERATIONS  = 5       # How many search→train cycles\n",
    "MAX_PATH_LENGTH = 800     # Reject training paths longer than this\n",
    "ENABLE_MCTS     = False   # MCTS is slow; keep False unless you have many hours\n",
    "MAX_NODES       = 100_000 # 100K nodes: ~25s per presentation, ~5hrs per iteration\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, DRIVE_DIR)\n",
    "\n",
    "print('Config set:')\n",
    "print(f'  max_iterations  = {MAX_ITERATIONS}')\n",
    "print(f'  max_path_length = {MAX_PATH_LENGTH}')\n",
    "print(f'  enable_mcts     = {ENABLE_MCTS}')\n",
    "print(f'  max_nodes       = {MAX_NODES:,}')\n",
    "\n",
    "\n",
    "# ── Progress streaming helper (used by Cells 6 and 7) ─────────────────────────\n",
    "def run_with_progress(cmd, cwd):\n",
    "    \"\"\"Run cmd as subprocess, printing headers normally and progress on one line.\"\"\"\n",
    "    import subprocess, sys, os\n",
    "\n",
    "    HEADER_PATTERNS = [\n",
    "        '===', 'ITERATION', '--- Step', 'complete:', 'REFINEMENT COMPLETE',\n",
    "        'ERROR', 'Traceback', 'File \"', 'Error:', 'Running:', 'Resuming:',\n",
    "        'Total presentations', 'Greedy baseline', 'Resumed from',\n",
    "        'No previous state', 'Loading greedy', 'Converged', 'Interrupted',\n",
    "        'State saved', '[idx=',\n",
    "    ]\n",
    "\n",
    "    env = os.environ.copy()\n",
    "    env['PYTHONUNBUFFERED'] = '1'  # force line-by-line flushing in subprocess\n",
    "\n",
    "    proc = subprocess.Popen(\n",
    "        cmd, cwd=cwd,\n",
    "        stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "        text=True, bufsize=1, env=env,\n",
    "    )\n",
    "\n",
    "    last_was_progress = False\n",
    "    for raw in proc.stdout:\n",
    "        line = raw.rstrip()\n",
    "        if not line:\n",
    "            continue\n",
    "        is_header = any(p in line for p in HEADER_PATTERNS)\n",
    "        if is_header:\n",
    "            if last_was_progress:\n",
    "                sys.stdout.write('\\n')\n",
    "            print(line)\n",
    "            last_was_progress = False\n",
    "        else:\n",
    "            sys.stdout.write(f'\\r  {line:<90}')\n",
    "            sys.stdout.flush()\n",
    "            last_was_progress = True\n",
    "\n",
    "    if last_was_progress:\n",
    "        sys.stdout.write('\\n')\n",
    "    proc.wait()\n",
    "    return proc.returncode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6 — Fresh Start\n",
    "Run this to begin a new refinement run from the greedy baseline. **Skip if resuming an existing run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: /usr/bin/python3 /content/drive/MyDrive/AC-Solver-Caltech/experiments/iterative_refinement.py --max-iterations 5 --max-path-length 300 --max-nodes 100000\n",
      "======================================================================\n",
      "    AC-Solver Iterative Refinement Pipeline                                                 \n",
      "======================================================================\n",
      "    State file:     /content/drive/MyDrive/AC-Solver-Caltech/experiments/refinement/refinement_state.json\n",
      "======================================================================\n",
      "  Total presentations: 1190\n",
      "  Loading greedy-solved paths as seed...\n",
      "  Greedy baseline: 533 solved\n",
      "======================================================================\n",
      "  ITERATION 0\n",
      "======================================================================\n",
      "    Model: /content/drive/MyDrive/AC-Solver-Caltech/value_search/checkpoints/best_mlp.pt (original, trained on greedy paths)\n",
      "  --- Step 1: Search ---\n",
      "    Searching 657/1190 unsolved presentations                                               \n",
      "  Running: /usr/bin/python3 /content/drive/MyDrive/AC-Solver-Caltech/experiments/run_experiments.py --config /content/drive/MyDrive/AC-Solver-Caltech/experiments/refinement/iter_0/iter_config.yaml --indices /content/drive/MyDrive/AC-Solver-Caltech/experiments/refinement/iter_0/unsolved_indices.txt\n",
      "============================================================\n",
      "    AC-Solver Experiment Runner                                                             \n",
      "============================================================\n",
      "    Started:    2026-02-26_04-06-25                                                         6-25ig.yaml\n",
      "============================================================\n",
      "  Loading presentations...                                                                  \n",
      "  Total presentations: 1190\n",
      "    --indices: running on 657/1190 presentations                                            \n",
      "============================================================\n",
      "    [3] V-GUIDED GREEDY (ours, mlp + cyclic reduce) — 100,000 nodes                         \n",
      "============================================================\n",
      "      V-Greedy: 200/657, solved=5, ETA 11.2h                                                "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'https://8080-m-hm-l0clbxvw5hnh-c.europe-west4-1.prod.colab.dev/'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "cmd = [\n",
    "    sys.executable,\n",
    "    os.path.join(DRIVE_DIR, 'experiments/iterative_refinement.py'),\n",
    "    '--max-iterations', str(MAX_ITERATIONS),\n",
    "    '--max-path-length', str(MAX_PATH_LENGTH),\n",
    "]\n",
    "if ENABLE_MCTS:\n",
    "    cmd.append('--enable-mcts')\n",
    "if MAX_NODES is not None:\n",
    "    cmd.extend(['--max-nodes', str(MAX_NODES)])\n",
    "\n",
    "print('Running:', ' '.join(cmd))\n",
    "rc = run_with_progress(cmd, cwd=DRIVE_DIR)\n",
    "print('Exit code:', rc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7 — Reconstruct State (run only if state files are missing)\n",
    "Run this **only** when `refinement_state.json` is missing but partial search results exist on Drive (e.g. after accidentally deleting state or after a very early crash before any state was saved).  \n",
    "It auto-detects the latest partial results directory and rebuilds both state files.  \n",
    "Then run Cell 8 to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using partial results dir: /content/drive/MyDrive/AC-Solver-Caltech/experiments/results/2026-02-26_04-06-25\n",
      "  Contains: 397 processed, 6 solved: [599, 689, 698, 711, 712, 834]\n",
      "Greedy paths loaded: 533\n",
      "Written: all_solved_paths.json (533 paths)\n",
      "Written: refinement_state.json\n",
      "\n",
      "Ready — run Cell 8 (Resume).\n"
     ]
    }
   ],
   "source": [
    "import os, sys, json\n",
    "\n",
    "sys.path.insert(0, DRIVE_DIR)\n",
    "from value_search.data_extraction import load_presentations, load_paths\n",
    "\n",
    "DATA_DIR       = os.path.join(DRIVE_DIR, 'ac_solver/search/miller_schupp/data')\n",
    "REFINEMENT_DIR = os.path.join(DRIVE_DIR, 'experiments/refinement')\n",
    "RESULTS_BASE   = os.path.join(DRIVE_DIR, 'experiments/results')\n",
    "os.makedirs(REFINEMENT_DIR, exist_ok=True)\n",
    "\n",
    "# 1. Auto-detect the latest partial results directory\n",
    "candidate_dirs = sorted([\n",
    "    d for d in os.listdir(RESULTS_BASE)\n",
    "    if os.path.isdir(os.path.join(RESULTS_BASE, d))\n",
    "    and os.path.exists(os.path.join(RESULTS_BASE, d, 'v_guided_greedy_progress.jsonl'))\n",
    "])\n",
    "if not candidate_dirs:\n",
    "    print(\"ERROR: No partial results found in\", RESULTS_BASE)\n",
    "    raise SystemExit\n",
    "\n",
    "PARTIAL_DIR = os.path.join(RESULTS_BASE, candidate_dirs[-1])\n",
    "print(f\"Using partial results dir: {PARTIAL_DIR}\")\n",
    "\n",
    "# Show what's in it\n",
    "jsonl = os.path.join(PARTIAL_DIR, 'v_guided_greedy_progress.jsonl')\n",
    "total, solved_indices = 0, []\n",
    "with open(jsonl) as f:\n",
    "    for line in f:\n",
    "        r = json.loads(line)\n",
    "        total += 1\n",
    "        if r.get('solved'):\n",
    "            solved_indices.append(r['idx'])\n",
    "print(f\"  Contains: {total} processed, {len(solved_indices)} solved: {solved_indices}\")\n",
    "\n",
    "# 2. Reload greedy paths from source data\n",
    "solved_pres = load_presentations(os.path.join(DATA_DIR, 'greedy_solved_presentations.txt'))\n",
    "raw_paths   = load_paths(os.path.join(DATA_DIR, 'greedy_search_paths.txt'))\n",
    "greedy_paths = {\n",
    "    tuple(pres): [[a - 1, l] for a, l in raw_path[1:]]\n",
    "    for pres, raw_path in zip(solved_pres, raw_paths)\n",
    "}\n",
    "print(f\"Greedy paths loaded: {len(greedy_paths)}\")\n",
    "\n",
    "# 3. Write all_solved_paths.json\n",
    "paths_file = os.path.join(REFINEMENT_DIR, 'all_solved_paths.json')\n",
    "with open(paths_file, 'w') as f:\n",
    "    json.dump({str(list(k)): v for k, v in greedy_paths.items()}, f)\n",
    "print(f\"Written: all_solved_paths.json ({len(greedy_paths)} paths)\")\n",
    "\n",
    "# 4. Write refinement_state.json pointing at partial results\n",
    "state = {\n",
    "    \"iteration\": 0,\n",
    "    \"solved_per_iteration\": [],\n",
    "    \"total_solved_per_iteration\": [len(greedy_paths)],\n",
    "    \"model_paths\": [],\n",
    "    \"results_dirs\": [PARTIAL_DIR],\n",
    "}\n",
    "state_file = os.path.join(REFINEMENT_DIR, 'refinement_state.json')\n",
    "with open(state_file, 'w') as f:\n",
    "    json.dump(state, f, indent=2)\n",
    "print(f\"Written: refinement_state.json\")\n",
    "print(f\"\\nReady — run Cell 8 (Resume).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8 — Resume After Disconnect\n",
    "After Colab disconnects: re-run Cells 1 → 2 → 3 → 5, then run this cell.  \n",
    "Picks up from the last completed iteration automatically — state persists on Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming: /usr/bin/python3 /content/drive/MyDrive/AC-Solver-Caltech/experiments/iterative_refinement.py --resume --max-iterations 5 --max-path-length 800 --max-nodes 100000\n",
      "======================================================================\n",
      "    AC-Solver Iterative Refinement Pipeline                                                 \n",
      "======================================================================\n",
      "    State file:     /content/drive/MyDrive/AC-Solver-Caltech/experiments/refinement/refinement_state.json\n",
      "======================================================================\n",
      "  Total presentations: 1190\n",
      "  Resumed from iteration 0\n",
      "    Previously solved: 533/1190                                                             \n",
      "======================================================================\n",
      "  ITERATION 0\n",
      "======================================================================\n",
      "    Model: /content/drive/MyDrive/AC-Solver-Caltech/value_search/checkpoints/best_mlp.pt (original, trained on greedy paths)\n",
      "  --- Step 1: Search ---\n",
      "    Resuming search from: /content/drive/MyDrive/AC-Solver-Caltech/experiments/results/2026-02-26_04-06-25\n",
      "  Running: /usr/bin/python3 /content/drive/MyDrive/AC-Solver-Caltech/experiments/run_experiments.py --config /content/drive/MyDrive/AC-Solver-Caltech/experiments/refinement/iter_0/iter_config.yaml --indices /content/drive/MyDrive/AC-Solver-Caltech/experiments/refinement/iter_0/unsolved_indices.txt --resume-dir /content/drive/MyDrive/AC-Solver-Caltech/experiments/results/2026-02-26_04-06-25\n",
      "============================================================\n",
      "    AC-Solver Experiment Runner                                                             \n",
      "============================================================\n",
      "    Started:    2026-02-26_04-06-25                                                         6-25ig.yaml\n",
      "============================================================\n",
      "  Loading presentations...                                                                  \n",
      "  Total presentations: 1190\n",
      "    --indices: running on 657/1190 presentations                                            \n",
      "============================================================\n",
      "    [3] V-GUIDED GREEDY (ours, mlp + cyclic reduce) — 100,000 nodes                         \n",
      "============================================================\n",
      "  Resuming: skipping 482 already processed\n",
      "  [idx=1015] unsolved, nodes=100001, t=229.5s | 1/175 solved=0 ETA 11.1h\n",
      "  [idx=1016] unsolved, nodes=100002, t=86.9s | 2/175 solved=0 ETA 7.6h\n",
      "  [idx=1017] unsolved, nodes=100000, t=71.5s | 3/175 solved=0 ETA 6.2h\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-185505/2122634444.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Resuming:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_with_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDRIVE_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Exit code:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-185505/2689809636.py\u001b[0m in \u001b[0;36mrun_with_progress\u001b[0;34m(cmd, cwd)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mlast_was_progress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mraw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cmd = [\n",
    "    sys.executable,\n",
    "    os.path.join(DRIVE_DIR, 'experiments/iterative_refinement.py'),\n",
    "    '--resume',\n",
    "    '--max-iterations', str(MAX_ITERATIONS),\n",
    "    '--max-path-length', str(MAX_PATH_LENGTH),\n",
    "]\n",
    "if ENABLE_MCTS:\n",
    "    cmd.append('--enable-mcts')\n",
    "if MAX_NODES is not None:\n",
    "    cmd.extend(['--max-nodes', str(MAX_NODES)])\n",
    "\n",
    "print('Resuming:', ' '.join(cmd))\n",
    "rc = run_with_progress(cmd, cwd=DRIVE_DIR)\n",
    "print('Exit code:', rc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 9 — Skip Iter 0 Search: Retrain & Advance to Iter 1\nRun this **instead of re-doing the iter 0 search** when the search is already complete\n(or close enough). It:\n1. Loads all solved paths (greedy seed + any solutions found in iter_0 results)\n2. Rebuilds training data with the correct `negative_label = 5 × MAX_PATH_LENGTH`\n3. Retrains both MLP and Seq models\n4. Advances state to `iteration=1` so Cell 8 (Resume) picks up from there"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import os, sys, json\nfrom ast import literal_eval\nsys.path.insert(0, DRIVE_DIR)\n\nfrom value_search.data_extraction import build_dataset_from_dict\nfrom value_search.benchmark import load_all_presentations\n\nREFINEMENT_DIR = os.path.join(DRIVE_DIR, 'experiments/refinement')\nall_presentations = load_all_presentations()\npres_by_idx = {i: tuple(p) for i, p in enumerate(all_presentations)}\n\n# 1. Load existing solved paths (greedy seed)\npaths_file = os.path.join(REFINEMENT_DIR, 'all_solved_paths.json')\nwith open(paths_file) as f:\n    raw = json.load(f)\nsolved_paths = {tuple(literal_eval(k)): v for k, v in raw.items()}\nprint(f'Loaded {len(solved_paths)} solved paths from all_solved_paths.json')\n\n# 2. Scan iter_0 search results and add any newly solved presentations\niter0_dir = os.path.join(REFINEMENT_DIR, 'iter_0')\nn_extra = 0\nif os.path.isdir(iter0_dir):\n    subdirs = sorted([\n        d for d in os.listdir(iter0_dir)\n        if os.path.isdir(os.path.join(iter0_dir, d)) and d[0].isdigit()\n    ])\n    if subdirs:\n        results_dir = os.path.join(iter0_dir, subdirs[-1])\n        jsonl_path = os.path.join(results_dir, 'v_guided_greedy_progress.jsonl')\n        if os.path.exists(jsonl_path):\n            with open(jsonl_path) as f:\n                for line in f:\n                    r = json.loads(line)\n                    if r.get('solved') and r.get('path'):\n                        pres_tuple = pres_by_idx[r['idx']]\n                        if pres_tuple not in solved_paths:\n                            solved_paths[pres_tuple] = r['path']\n                            n_extra += 1\n            print(f'Added {n_extra} new solutions from iter_0 search results')\n\n# 3. Save updated solved paths\nwith open(paths_file, 'w') as f:\n    json.dump({str(list(k)): v for k, v in solved_paths.items()}, f)\nprint(f'Total solved paths: {len(solved_paths)}')\n\n# 4. Build training data with corrected negative label (5x, not 2x)\nnegative_label = float(MAX_PATH_LENGTH * 5)\ndata_path = os.path.join(REFINEMENT_DIR, 'training_data_iter_0.pkl')\nbuild_dataset_from_dict(\n    solved_paths=solved_paths,\n    all_presentations=all_presentations,\n    output_path=data_path,\n    negative_label=negative_label,\n    max_path_length=MAX_PATH_LENGTH,\n)\nprint(f'Training data built with negative_label={negative_label}')\n\n# 5. Retrain both MLP and Seq\ncheckpoint_dir = os.path.join(REFINEMENT_DIR, 'checkpoints_iter_0')\ncmd = [\n    sys.executable,\n    os.path.join(DRIVE_DIR, 'value_search/train_value_net.py'),\n    '--data-path', data_path,\n    '--save-dir', checkpoint_dir,\n    '--architecture', 'both',\n    '--epochs', '100',\n]\nprint('Training...')\nrc = run_with_progress(cmd, cwd=DRIVE_DIR)\nif rc != 0:\n    print(f'ERROR: training failed (exit code {rc})')\n    raise SystemExit\n\n# 6. Verify checkpoint\nckpt = os.path.join(checkpoint_dir, 'best_mlp.pt')\nif not os.path.exists(ckpt):\n    print('ERROR: checkpoint not found!')\n    raise SystemExit\n\n# 7. Advance state to iteration=1\nstate_file = os.path.join(REFINEMENT_DIR, 'refinement_state.json')\nwith open(state_file) as f:\n    state = json.load(f)\n\nprev_total = state['total_solved_per_iteration'][-1] if state['total_solved_per_iteration'] else 0\ntotal_now = len(solved_paths)\nnew_count = total_now - prev_total\nstate['model_paths'].append(ckpt)\nstate['iteration'] = 1\nstate['solved_per_iteration'].append(new_count)\nstate['total_solved_per_iteration'].append(total_now)\n\nwith open(state_file, 'w') as f:\n    json.dump(state, f, indent=2)\n\nprint(f'\\nDone! State → iteration=1, total_solved={total_now} (+{new_count} new)')\nprint('Run Cell 8 (Resume) to continue from iteration 1.')\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10 — Check Progress Anytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n\nstate_file = os.path.join(DRIVE_DIR, 'experiments/refinement/refinement_state.json')\nif not os.path.exists(state_file):\n    print('No state file — run Cell 6 (fresh start) or Cell 7 (reconstruct state) first.')\nelse:\n    with open(state_file) as f:\n        state = json.load(f)\n\n    paths_file = os.path.join(DRIVE_DIR, 'experiments/refinement/all_solved_paths.json')\n    total_solved = 0\n    if os.path.exists(paths_file):\n        with open(paths_file) as f:\n            total_solved = len(json.load(f))\n\n    print(f'Current iteration:  {state[\"iteration\"]}')\n    print(f'Total solved:       {total_solved}/1190')\n    print(f'Solved per iter:    {state[\"solved_per_iteration\"]}')\n    print(f'Total per iter:     {state[\"total_solved_per_iteration\"]}')\n\n    refinement_dir = os.path.join(DRIVE_DIR, 'experiments/refinement')\n    print(f'\\nFiles in refinement dir:')\n    for f in sorted(os.listdir(refinement_dir)):\n        fpath = os.path.join(refinement_dir, f)\n        size_mb = os.path.getsize(fpath) / 1e6 if os.path.isfile(fpath) else 0\n        tag = f'({size_mb:.1f} MB)' if os.path.isfile(fpath) else '(dir)'\n        print(f'  {f}  {tag}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}