{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AC-Solver Iterative Refinement — Colab Runner\n",
    "\n",
    "**Runtime recommendation: High-RAM CPU**  \n",
    "The search (V-guided greedy, beam) is CPU-only. GPU only helps training, which takes ~10 min per iteration — not the bottleneck.\n",
    "\n",
    "**Persistence:** Repo lives on Google Drive. If Colab disconnects, re-run cells 1–3 (fast), then Cell 6 to resume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1 — Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Drive mounted.\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print('Drive mounted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2 — Clone or Update Repo on Drive\n",
    "\n",
    "**First time only:** enter your GitHub Personal Access Token when prompted.  \n",
    "Get one at: GitHub → Settings → Developer Settings → Personal access tokens → Fine-grained → repo read access.  \n",
    "After first clone, this cell just does `git pull`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating 0998ef5..e865cf2\n",
      "Fast-forward\n",
      " experiments/colab_iterative_refinement.ipynb | 37 ++--------------------------\n",
      " 1 file changed, 2 insertions(+), 35 deletions(-)\n",
      "\n",
      "Working directory: /content/drive/MyDrive/AC-Solver-Caltech\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "REPO_URL = 'https://github.com/Avi161/AC-Solver-Caltech.git'\n",
    "DRIVE_DIR = '/content/drive/MyDrive/AC-Solver-Caltech'\n",
    "BRANCH = 'feat/test-new-exp-claude'\n",
    "\n",
    "if not os.path.exists(DRIVE_DIR):\n",
    "    # First time: clone with token\n",
    "    from getpass import getpass\n",
    "    token = getpass('GitHub Personal Access Token: ')\n",
    "    auth_url = REPO_URL.replace('https://', f'https://{token}@')\n",
    "    subprocess.run(['git', 'clone', auth_url, DRIVE_DIR], check=True)\n",
    "    subprocess.run(['git', 'checkout', BRANCH], cwd=DRIVE_DIR, check=True)\n",
    "    print('Repo cloned to Drive.')\n",
    "else:\n",
    "    # Already cloned: checkout branch first, then pull\n",
    "    subprocess.run(['git', 'checkout', BRANCH], cwd=DRIVE_DIR, check=True)\n",
    "    result = subprocess.run(['git', 'pull'], cwd=DRIVE_DIR, capture_output=True, text=True)\n",
    "    print(result.stdout or 'Already up to date.')\n",
    "\n",
    "os.chdir(DRIVE_DIR)\n",
    "print(f'Working directory: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3 — Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.0/61.0 kB 2.3 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.0/18.0 MB 138.5 MB/s eta 0:00:00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "result = subprocess.run(\n",
    "    ['pip', 'install', '-q', '-r', 'requirements.txt'],\n",
    "    cwd=DRIVE_DIR, capture_output=True, text=True\n",
    ")\n",
    "print(result.stdout[-2000:] if result.stdout else 'Done.')\n",
    "if result.returncode != 0:\n",
    "    print('ERRORS:', result.stderr[-1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4 — Verify Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓  Original model checkpoint\n",
      "  ✓  Feature stats\n",
      "  ✓  Greedy solved presentations\n",
      "  ✓  Greedy search paths\n",
      "  ✓  Config\n",
      "\n",
      "  No saved state — will start fresh.\n",
      "\n",
      "All checks passed. Ready to run.\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "\n",
    "checks = {\n",
    "    'Original model checkpoint':  'value_search/checkpoints/best_mlp.pt',\n",
    "    'Feature stats':              'value_search/checkpoints/feature_stats.json',\n",
    "    'Greedy solved presentations': 'ac_solver/search/miller_schupp/data/greedy_solved_presentations.txt',\n",
    "    'Greedy search paths':         'ac_solver/search/miller_schupp/data/greedy_search_paths.txt',\n",
    "    'Config':                      'experiments/config.yaml',\n",
    "}\n",
    "\n",
    "all_ok = True\n",
    "for label, path in checks.items():\n",
    "    full = os.path.join(DRIVE_DIR, path)\n",
    "    exists = os.path.exists(full)\n",
    "    status = '✓' if exists else '✗  MISSING'\n",
    "    print(f'  {status}  {label}')\n",
    "    if not exists:\n",
    "        all_ok = False\n",
    "\n",
    "# Check refinement state (if resuming)\n",
    "state_file = os.path.join(DRIVE_DIR, 'experiments/refinement/refinement_state.json')\n",
    "if os.path.exists(state_file):\n",
    "    with open(state_file) as f:\n",
    "        state = json.load(f)\n",
    "    print(f'\\n  Saved state found: iteration={state[\"iteration\"]}, '\n",
    "          f'total_solved_history={state[\"total_solved_per_iteration\"]}')\n",
    "else:\n",
    "    print('\\n  No saved state — will start fresh.')\n",
    "\n",
    "if all_ok:\n",
    "    print('\\nAll checks passed. Ready to run.')\n",
    "else:\n",
    "    print('\\nFix missing files before running.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5 — Configuration\n",
    "Edit these before running. Then run Cell 6 (fresh start) or Cell 7 (resume)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Tune these ────────────────────────────────────────────────────────────────\nMAX_ITERATIONS  = 5       # How many search→train cycles\nMAX_PATH_LENGTH = 300     # Reject training paths longer than this\nENABLE_MCTS     = False   # MCTS is slow; keep False unless you have many hours\n\n# Set to an integer (e.g. 100_000) to cap search budget for a quick smoke test.\n# Set to None for full 1M-node search.\nMAX_NODES = None\n# ──────────────────────────────────────────────────────────────────────────────\n\nimport sys\nsys.path.insert(0, DRIVE_DIR)\n\nprint('Config set:')\nprint(f'  max_iterations  = {MAX_ITERATIONS}')\nprint(f'  max_path_length = {MAX_PATH_LENGTH}')\nprint(f'  enable_mcts     = {ENABLE_MCTS}')\nprint(f'  max_nodes       = {MAX_NODES if MAX_NODES else \"1,000,000 (default)\"}')\n\n\n# ── Progress streaming helper (used by Cells 6 and 7) ─────────────────────────\ndef run_with_progress(cmd, cwd):\n    \"\"\"Run cmd as subprocess, printing headers normally and progress on one line.\"\"\"\n    import subprocess, sys, os\n\n    HEADER_PATTERNS = [\n        '===', 'ITERATION', '--- Step', 'complete:', 'REFINEMENT COMPLETE',\n        'ERROR', 'Traceback', 'File \"', 'Error:', 'Running:', 'Resuming:',\n        'Total presentations', 'Greedy baseline', 'Resumed from',\n        'No previous state', 'Loading greedy', 'Converged', 'Interrupted',\n        'State saved',\n    ]\n\n    env = os.environ.copy()\n    env['PYTHONUNBUFFERED'] = '1'  # force line-by-line flushing in subprocess\n\n    proc = subprocess.Popen(\n        cmd, cwd=cwd,\n        stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n        text=True, bufsize=1, env=env,\n    )\n\n    last_was_progress = False\n    for raw in proc.stdout:\n        line = raw.rstrip()\n        if not line:\n            continue\n        is_header = any(p in line for p in HEADER_PATTERNS)\n        if is_header:\n            if last_was_progress:\n                sys.stdout.write('\\n')\n            print(line)\n            last_was_progress = False\n        else:\n            sys.stdout.write(f'\\r  {line:<90}')\n            sys.stdout.flush()\n            last_was_progress = True\n\n    if last_was_progress:\n        sys.stdout.write('\\n')\n    proc.wait()\n    return proc.returncode"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6 — Fresh Start\n",
    "Run this if you have no previous state (first time ever).  \n",
    "**Skip this if resuming — use Cell 7 instead.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "cmd = [\n    sys.executable,\n    os.path.join(DRIVE_DIR, 'experiments/iterative_refinement.py'),\n    '--max-iterations', str(MAX_ITERATIONS),\n    '--max-path-length', str(MAX_PATH_LENGTH),\n]\nif ENABLE_MCTS:\n    cmd.append('--enable-mcts')\nif MAX_NODES is not None:\n    cmd.extend(['--max-nodes', str(MAX_NODES)])\n\nprint('Running:', ' '.join(cmd))\nrc = run_with_progress(cmd, cwd=DRIVE_DIR)\nprint('Exit code:', rc)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7 — Resume After Disconnect\n",
    "After Colab disconnects: re-run Cells 1–3 (takes ~1 min), configure Cell 5, then run this cell.  \n",
    "It picks up from the last completed iteration automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "cmd = [\n    sys.executable,\n    os.path.join(DRIVE_DIR, 'experiments/iterative_refinement.py'),\n    '--resume',\n    '--max-iterations', str(MAX_ITERATIONS),\n    '--max-path-length', str(MAX_PATH_LENGTH),\n]\nif ENABLE_MCTS:\n    cmd.append('--enable-mcts')\nif MAX_NODES is not None:\n    cmd.extend(['--max-nodes', str(MAX_NODES)])\n\nprint('Resuming:', ' '.join(cmd))\nrc = run_with_progress(cmd, cwd=DRIVE_DIR)\nprint('Exit code:', rc)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8 — Check Progress Anytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "state_file = os.path.join(DRIVE_DIR, 'experiments/refinement/refinement_state.json')\n",
    "if not os.path.exists(state_file):\n",
    "    print('No state file yet — run Cell 6 first.')\n",
    "else:\n",
    "    with open(state_file) as f:\n",
    "        state = json.load(f)\n",
    "\n",
    "    paths_file = os.path.join(DRIVE_DIR, 'experiments/refinement/all_solved_paths.json')\n",
    "    total_solved = 0\n",
    "    if os.path.exists(paths_file):\n",
    "        with open(paths_file) as f:\n",
    "            total_solved = len(json.load(f))\n",
    "\n",
    "    print(f'Current iteration:  {state[\"iteration\"]}')\n",
    "    print(f'Total solved:       {total_solved}/1190')\n",
    "    print(f'Solved per iter:    {state[\"solved_per_iteration\"]}')\n",
    "    print(f'Total per iter:     {state[\"total_solved_per_iteration\"]}')\n",
    "\n",
    "    refinement_dir = os.path.join(DRIVE_DIR, 'experiments/refinement')\n",
    "    print(f'\\nFiles in refinement dir:')\n",
    "    for f in sorted(os.listdir(refinement_dir)):\n",
    "        fpath = os.path.join(refinement_dir, f)\n",
    "        size_mb = os.path.getsize(fpath) / 1e6 if os.path.isfile(fpath) else 0\n",
    "        tag = f'({size_mb:.1f} MB)' if os.path.isfile(fpath) else '(dir)'\n",
    "        print(f'  {f}  {tag}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}