{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# AC-Solver: Parallelized Value-Guided Search on A100\n",
    "\n",
    "This notebook runs V-guided greedy, beam search, and MCTS on **all 1190 Miller-Schupp presentations** using multiprocessing + A100 GPU.\n",
    "\n",
    "**Runtime setup**: Go to `Runtime > Change runtime type > A100 GPU`\n",
    "\n",
    "**Expected runtime**: ~30-60 min for V-guided at 1M nodes, ~2-4h for all three algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup: Clone Repo and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf755952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "clone-repo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'AC-Solver-Caltech'...\n",
      "remote: Enumerating objects: 1167, done.\u001b[K\n",
      "remote: Counting objects: 100% (180/180), done.\u001b[K\n",
      "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
      "remote: Total 1167 (delta 147), reused 138 (delta 130), pack-reused 987 (from 1)\u001b[K\n",
      "Receiving objects: 100% (1167/1167), 9.17 MiB | 10.48 MiB/s, done.\n",
      "Resolving deltas: 100% (598/598), done.\n",
      "/content/AC-Solver-Caltech\n",
      "error: pathspec 'feat/test-antigravity' did not match any file(s) known to git\n"
     ]
    }
   ],
   "source": [
    "# Clone the repo (change to your fork URL if needed)\n",
    "!git clone https://github.com/Avi161/AC-Solver-Caltech.git\n",
    "%cd AC-Solver-Caltech\n",
    "!git checkout feat/test-antigravity\n",
    "!pip install -q torch numpy pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "verify-gpu",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "GPU Memory: 85.1 GB\n",
      "CPU cores: 12\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected! Go to Runtime > Change runtime type > A100 GPU\")\n",
    "\n",
    "import multiprocessing\n",
    "print(f\"CPU cores: {multiprocessing.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "verify-checkpoints",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ value_search/checkpoints/best_mlp.pt (405 KB)\n",
      "  ✓ value_search/checkpoints/best_seq.pt (159 KB)\n",
      "  ✓ value_search/checkpoints/feature_stats.json (1 KB)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Verify that model checkpoints exist\n",
    "for f in ['value_search/checkpoints/best_mlp.pt',\n",
    "          'value_search/checkpoints/best_seq.pt',\n",
    "          'value_search/checkpoints/feature_stats.json']:\n",
    "    exists = os.path.exists(f)\n",
    "    size = os.path.getsize(f) if exists else 0\n",
    "    print(f\"  {'✓' if exists else '✗'} {f} ({size/1024:.0f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Adjust these settings based on how long you want to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: algorithm=all, arch=mlp, max_nodes=1,000,000, workers=8\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION — adjust these as needed\n",
    "# ============================================================\n",
    "\n",
    "# Which algorithms to run. Options: 'v_guided', 'beam', 'mcts', or 'all'\n",
    "ALGORITHM = 'all'\n",
    "\n",
    "# Value network architecture: 'mlp' (faster) or 'seq' (more accurate)\n",
    "ARCHITECTURE = 'mlp'\n",
    "\n",
    "# Max nodes to explore per presentation per algorithm.\n",
    "# Recommended budgets:\n",
    "#   1,000   -> ~30s total, quick sanity check\n",
    "#   10,000  -> ~5 min total\n",
    "#   100,000 -> ~30-60 min for v_guided, much longer for MCTS\n",
    "#   1,000,000 -> ~4-8h for v_guided (best results)\n",
    "MAX_NODES = 1_000_000\n",
    "\n",
    "# Number of parallel worker processes.\n",
    "# A100 Colab has ~12 CPU cores. Workers run search in parallel.\n",
    "# Each worker loads its own copy of the model onto the GPU.\n",
    "# Recommended: 6-8 for A100 (balance CPU search + GPU inference)\n",
    "NUM_WORKERS = 8\n",
    "\n",
    "# Beam width (only for beam search)\n",
    "BEAM_WIDTH = 50\n",
    "\n",
    "# MCTS exploration constant\n",
    "C_EXPLORE = 1.41\n",
    "\n",
    "# Apply cyclic reduction after AC moves (slightly better results)\n",
    "CYCLICALLY_REDUCE = True\n",
    "\n",
    "print(f\"Config: algorithm={ALGORITHM}, arch={ARCHITECTURE}, \"\n",
    "      f\"max_nodes={MAX_NODES:,}, workers={NUM_WORKERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-header",
   "metadata": {},
   "source": [
    "## 3. Run Parallel Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import multiprocessing\n",
    "multiprocessing.set_start_method('spawn', force=True)\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from ac_solver.envs.ac_moves import ACMove\n",
    "from ac_solver.envs.utils import is_presentation_trivial\n",
    "from value_search.value_guided_search import (\n",
    "    value_guided_greedy_search, beam_search, load_model,\n",
    "    backfill_solution_cache,\n",
    ")\n",
    "from value_search.mcts import mcts_search\n",
    "\n",
    "print(\"Imports OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "load-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total presentations: 1190\n",
      "Greedy solved: 533\n",
      "Unsolved by greedy: 657\n"
     ]
    }
   ],
   "source": [
    "# Load all presentations\n",
    "def load_presentations(path):\n",
    "    pres = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                pres.append(np.array(literal_eval(line.strip()), dtype=np.int8))\n",
    "    return pres\n",
    "\n",
    "all_pres = load_presentations('ac_solver/search/miller_schupp/data/all_presentations.txt')\n",
    "greedy_solved_pres = load_presentations('ac_solver/search/miller_schupp/data/greedy_solved_presentations.txt')\n",
    "greedy_solved_set = set(tuple(p) for p in greedy_solved_pres)\n",
    "\n",
    "print(f\"Total presentations: {len(all_pres)}\")\n",
    "print(f\"Greedy solved: {len(greedy_solved_set)}\")\n",
    "print(f\"Unsolved by greedy: {len(all_pres) - len(greedy_solved_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "worker-func",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker function defined\n"
     ]
    }
   ],
   "source": [
    "def search_worker(args):\n",
    "    \"\"\"Search a single presentation. Runs in a subprocess.\"\"\"\n",
    "    idx, pres_list, algorithm, config = args\n",
    "    pres = np.array(pres_list, dtype=np.int8)\n",
    "    \n",
    "    # Each worker loads model independently (avoids CUDA fork issues)\n",
    "    device = config['device']\n",
    "    model, feat_mean, feat_std = load_model(\n",
    "        config['checkpoint'], config['architecture'],\n",
    "        config['feature_stats'], device\n",
    "    )\n",
    "    \n",
    "    t0 = time.time()\n",
    "    if algorithm == 'v_guided':\n",
    "        solved, path, stats = value_guided_greedy_search(\n",
    "            pres, model=model, architecture=config['architecture'],\n",
    "            feat_mean=feat_mean, feat_std=feat_std,\n",
    "            max_nodes_to_explore=config['max_nodes'], device=device,\n",
    "            cyclically_reduce_after_moves=config.get('cyclically_reduce', False),\n",
    "        )\n",
    "    elif algorithm == 'beam':\n",
    "        solved, path, stats = beam_search(\n",
    "            pres, model=model, architecture=config['architecture'],\n",
    "            feat_mean=feat_mean, feat_std=feat_std,\n",
    "            beam_width=config.get('beam_width', 50),\n",
    "            max_nodes_to_explore=config['max_nodes'], device=device,\n",
    "        )\n",
    "    elif algorithm == 'mcts':\n",
    "        solved, path, stats = mcts_search(\n",
    "            pres, model=model, architecture=config['architecture'],\n",
    "            feat_mean=feat_mean, feat_std=feat_std,\n",
    "            max_nodes_to_explore=config['max_nodes'],\n",
    "            c_explore=config.get('c_explore', 1.41), device=device,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown algorithm: {algorithm}\")\n",
    "    \n",
    "    elapsed = time.time() - t0\n",
    "    result = {\n",
    "        'idx': idx, 'solved': solved,\n",
    "        'path_length': len(path) if solved else 0,\n",
    "        'nodes_explored': stats.get('nodes_explored', 0),\n",
    "        'time': elapsed,\n",
    "    }\n",
    "    if solved and path:\n",
    "        result['path'] = [[int(a), int(l)] for a, l in path]\n",
    "    return result\n",
    "\n",
    "print(\"Worker function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-search",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Algorithms to run: ['v_guided', 'beam', 'mcts']\n",
      "Starting...\n",
      "\n",
      "============================================================\n",
      "  V_GUIDED | 1,000,000 nodes | 8 workers\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def run_parallel(presentations, algorithm, config, num_workers, greedy_solved_set):\n",
    "    \"\"\"Run search in parallel across all presentations.\"\"\"\n",
    "    n = len(presentations)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  {algorithm.upper()} | {config['max_nodes']:,} nodes | {num_workers} workers\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    work_items = [\n",
    "        (i, pres.tolist(), algorithm, config)\n",
    "        for i, pres in enumerate(presentations)\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    solved_count = 0\n",
    "    newly_solved_count = 0\n",
    "    t_start = time.time()\n",
    "    \n",
    "    with Pool(processes=num_workers) as pool:\n",
    "        for result in pool.imap_unordered(search_worker, work_items):\n",
    "            results.append(result)\n",
    "            if result['solved']:\n",
    "                solved_count += 1\n",
    "                if tuple(presentations[result['idx']]) not in greedy_solved_set:\n",
    "                    newly_solved_count += 1\n",
    "            \n",
    "            done = len(results)\n",
    "            if done % 1 == 0 or done == n:\n",
    "                elapsed = time.time() - t_start\n",
    "                rate = elapsed / done\n",
    "                eta = rate * (n - done)\n",
    "                eta_s = f\"{eta/60:.1f}m\" if eta < 3600 else f\"{eta/3600:.1f}h\"\n",
    "                print(f\"  {done}/{n} | solved={solved_count} | \"\n",
    "                      f\"new={newly_solved_count} | ETA {eta_s}\")\n",
    "    \n",
    "    total_time = time.time() - t_start\n",
    "    results.sort(key=lambda r: r['idx'])\n",
    "    \n",
    "    solved_results = [r for r in results if r['solved']]\n",
    "    path_lengths = [r['path_length'] for r in solved_results]\n",
    "    \n",
    "    print(f\"\\n  RESULT: {len(solved_results)}/{n} solved \"\n",
    "          f\"({newly_solved_count} new beyond greedy) in {total_time/60:.1f}m\")\n",
    "    if path_lengths:\n",
    "        print(f\"  Avg path: {np.mean(path_lengths):.1f}, \"\n",
    "              f\"Max: {max(path_lengths)}, Min: {min(path_lengths)}\")\n",
    "    \n",
    "    # Show newly solved\n",
    "    newly = [r for r in solved_results \n",
    "             if tuple(presentations[r['idx']]) not in greedy_solved_set]\n",
    "    if newly:\n",
    "        print(f\"\\n  *** NEWLY SOLVED ({len(newly)} presentations) ***\")\n",
    "        for r in newly[:30]:\n",
    "            pres = presentations[r['idx']]\n",
    "            mrl = len(pres) // 2\n",
    "            tl = int(np.count_nonzero(pres[:mrl]) + np.count_nonzero(pres[mrl:]))\n",
    "            print(f\"    idx={r['idx']:>4d}, path_len={r['path_length']:>3d}, \"\n",
    "                  f\"word_len={tl}, nodes={r['nodes_explored']:>7d}\")\n",
    "        if len(newly) > 30:\n",
    "            print(f\"    ... and {len(newly)-30} more\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Build config\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "arch = ARCHITECTURE\n",
    "ckpt = f'value_search/checkpoints/best_{arch}.pt'\n",
    "stats = 'value_search/checkpoints/feature_stats.json'\n",
    "\n",
    "config = {\n",
    "    'device': device,\n",
    "    'architecture': arch,\n",
    "    'checkpoint': ckpt,\n",
    "    'feature_stats': stats,\n",
    "    'max_nodes': MAX_NODES,\n",
    "    'beam_width': BEAM_WIDTH,\n",
    "    'c_explore': C_EXPLORE,\n",
    "    'cyclically_reduce': CYCLICALLY_REDUCE,\n",
    "}\n",
    "\n",
    "# Determine algorithms\n",
    "if ALGORITHM == 'all':\n",
    "    algos = ['v_guided', 'beam', 'mcts']\n",
    "else:\n",
    "    algos = [ALGORITHM]\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Algorithms to run: {algos}\")\n",
    "print(f\"Starting...\")\n",
    "\n",
    "all_results = {}\n",
    "all_solved_sets = {}\n",
    "\n",
    "for algo in algos:\n",
    "    results = run_parallel(all_pres, algo, config, NUM_WORKERS, greedy_solved_set)\n",
    "    all_results[algo] = results\n",
    "    all_solved_sets[algo] = set(r['idx'] for r in results if r['solved'])\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"  ALL DONE\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis-header",
   "metadata": {},
   "source": [
    "## 4. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "print(f\"{'Algorithm':<20} | {'Solved':>10} | {'New':>5} | {'Avg Path':>10} | {'Max Path':>10}\")\n",
    "print(f\"{'-'*20}-+-{'-'*10}-+-{'-'*5}-+-{'-'*10}-+-{'-'*10}\")\n",
    "\n",
    "for algo, results in all_results.items():\n",
    "    solved = [r for r in results if r['solved']]\n",
    "    newly = [r for r in solved if tuple(all_pres[r['idx']]) not in greedy_solved_set]\n",
    "    paths = [r['path_length'] for r in solved]\n",
    "    avg_p = f\"{np.mean(paths):.1f}\" if paths else \"—\"\n",
    "    max_p = f\"{max(paths)}\" if paths else \"—\"\n",
    "    print(f\"{algo:<20} | {f'{len(solved)}/1190':>10} | {len(newly):>5} | {avg_p:>10} | {max_p:>10}\")\n",
    "\n",
    "# Union\n",
    "if len(all_solved_sets) > 1:\n",
    "    union = set()\n",
    "    for s in all_solved_sets.values():\n",
    "        union |= s\n",
    "    union_new = sum(1 for idx in union if tuple(all_pres[idx]) not in greedy_solved_set)\n",
    "    print(f\"{'UNION':<20} | {f'{len(union)}/1190':>10} | {union_new:>5} |\")\n",
    "    \n",
    "    # Uniquely solved by each\n",
    "    print(f\"\\nUniquely solved:\")\n",
    "    for algo, s in all_solved_sets.items():\n",
    "        others = set().union(*(v for k, v in all_solved_sets.items() if k != algo))\n",
    "        unique = s - others\n",
    "        if unique:\n",
    "            print(f\"  {algo}: {len(unique)} unique\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "output_dir = f'experiments/results/{timestamp}_colab'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save config\n",
    "with open(f'{output_dir}/config.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'algorithms': algos,\n",
    "        'max_nodes': MAX_NODES,\n",
    "        'workers': NUM_WORKERS,\n",
    "        'architecture': ARCHITECTURE,\n",
    "        'beam_width': BEAM_WIDTH,\n",
    "        'c_explore': C_EXPLORE,\n",
    "        'device': device,\n",
    "    }, f, indent=2)\n",
    "\n",
    "# Save per-algorithm results\n",
    "for algo, results in all_results.items():\n",
    "    with open(f'{output_dir}/{algo}_results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "# Save newly solved summary\n",
    "all_newly = {}\n",
    "for algo, results in all_results.items():\n",
    "    newly = [r for r in results if r['solved'] \n",
    "             and tuple(all_pres[r['idx']]) not in greedy_solved_set]\n",
    "    all_newly[algo] = [r['idx'] for r in newly]\n",
    "\n",
    "with open(f'{output_dir}/newly_solved.json', 'w') as f:\n",
    "    json.dump(all_newly, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {output_dir}/\")\n",
    "print(f\"Files:\")\n",
    "for fname in sorted(os.listdir(output_dir)):\n",
    "    size = os.path.getsize(f'{output_dir}/{fname}')\n",
    "    print(f\"  {fname} ({size/1024:.0f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-header",
   "metadata": {},
   "source": [
    "## 5. Verify Newly Solved Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-paths",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that all reported solutions are actually correct\n",
    "def verify_solution(pres, path):\n",
    "    \"\"\"Replay path and verify it reaches trivial.\"\"\"\n",
    "    state = np.array(pres, dtype=np.int8)\n",
    "    mrl = len(state) // 2\n",
    "    wl = [int(np.count_nonzero(state[:mrl])), int(np.count_nonzero(state[mrl:]))]\n",
    "    for action, expected_len in path:\n",
    "        state, wl = ACMove(action, state, mrl, wl, cyclical=False)\n",
    "    return is_presentation_trivial(state)\n",
    "\n",
    "print(\"Verifying all solutions...\")\n",
    "errors = 0\n",
    "verified = 0\n",
    "for algo, results in all_results.items():\n",
    "    for r in results:\n",
    "        if r['solved'] and 'path' in r:\n",
    "            path = [(a, l) for a, l in r['path']]\n",
    "            ok = verify_solution(all_pres[r['idx']], path)\n",
    "            if not ok:\n",
    "                print(f\"  ERROR: {algo} idx={r['idx']} path does NOT reach trivial!\")\n",
    "                errors += 1\n",
    "            verified += 1\n",
    "\n",
    "print(f\"Verified {verified} solutions, {errors} errors\")\n",
    "if errors == 0:\n",
    "    print(\"All solutions are verified correct! ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "download-header",
   "metadata": {},
   "source": [
    "## 6. Download Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip results for download\n",
    "import shutil\n",
    "zip_path = shutil.make_archive(f'/content/ac_solver_results_{timestamp}', 'zip', output_dir)\n",
    "print(f\"Results zipped to: {zip_path}\")\n",
    "\n",
    "# If running on Colab, offer download\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(zip_path)\n",
    "except ImportError:\n",
    "    print(\"Not on Colab — download manually from the file browser.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
